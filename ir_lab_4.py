# -*- coding: utf-8 -*-
"""IR Lab 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LS5A1d_waC35WZ9u4wsGMLC8SvDxcahD

# **Information Retrieval System**

*   Develop an Information Retrieval System for any domain
*   The goal of this assignment is to develop search engines for any specific domain (music, health care,
education, etc.)
*   The search engine development should include:
1. Pre-processing of data
2. Building indexes
3. Compression of indexes
4. IR- models â€“ VSM, BM 25
5. Query mapping
6. Display of Results (top k results)
7. Explainable results
*   For example, build a search engine for searching song lyrics.
*   Dataset: https://labrosa.ee.columbia.edu/millionsong/musixmatch

### **Pre-Processing of Data**
"""

import string
import nltk

nltk.download('stopwords')
stemmer = nltk.stem.PorterStemmer()

class DocHandler:
    def __init__(self, name, text):
        self.name = name
        self.text = text
        self.preProcess()

    def getName(self):
        return self.name

    def getText(self):
        return self.text

    def getPreProcessed(self):
        return self.processed

    def getFrequency(self):
        return self.freq

    def setTFIDF(self, tfidf):
        self.tfidf = tfidf

    def getTFIDF(self):
        return self.tfidf

    def preProcess(self):
        self.removePunc()
        # print()
        self.toLower()
        # print()
        self.toTokens()
        # print()
        self.stopWordRemoval()
        # print()
        self.stemming()
        # print()
        self.frequency()
        # print()

    def removePunc(self):
        # print("Removing Punctuation")
        self.text = self.text.translate(str.maketrans('', '', string.punctuation))
        # print(self.text)

    def toLower(self):
        # print("Converting to Lowercase")
        self.text = self.text.lower()
        # print(self.text)

    def toTokens(self):
        # print("Converting to Tokens")
        self.tokens = self.text.split()
        # print(self.tokens)

    def stopWordRemoval(self):
        # print("Removing Stop Words")
        stop_words = set(nltk.corpus.stopwords.words('english'))
        self.tokens = [word for word in self.tokens if word not in stop_words]
        # print(self.tokens)

    def stemming(self):
        # print("Stemming")
        self.processed = [stemmer.stem(word) for word in self.tokens]
        # print(self.processed)

    def frequency(self):
        # print("Frequency")
        self.freq = {}
        for word in self.processed:
            if word in self.freq:
                self.freq[word] += 1
            else:
                self.freq[word] = 1
        # print(self.freq)

from types import prepare_class
documents = {'d1': 'Pointer is going to go to the gym from today!',
             'd2': 'Kaamesh ate chicken biriyani last week, when he went out with his friends.',
             'd3': 'India might win a silver or gold medal, thanks to Vinesh Phogat!',
             'd4': 'This is the last sentence; I don\'t know what else to type',
             'd5': 'Kaamesh is good at studies and sports!'}

docs = []
for doc in documents:
    docs.append(DocHandler(doc, documents[doc]))

pDocs = [obj.getPreProcessed() for obj in docs]

for doc in pDocs:
    print(doc)

print("Enter the query: ")
query = input()
query = DocHandler('Query', query)
pQuery = query.getPreProcessed()
print(pQuery)

"""### **Building Indexes**"""

def cosineSim(D, Q):
    numerator = 0
    for i in range(len(D)):
        numerator += D[i] * Q[i]
    denominatorD = 0
    for i in range(len(D)):
        denominatorD += D[i] * D[i]
    denominatorQ = 0
    for i in range(len(Q)):
        denominatorQ += Q[i] * Q[i]
    denominator = (denominatorD * denominatorQ) ** 0.5
    return numerator / denominator

import math

uniqueTerms = []
for doc in pDocs:
    for term in doc:
        if term not in uniqueTerms:
            uniqueTerms.append(term)

N = len(docs)
df = {}
for term in uniqueTerms:
    df[term] = 0
    for pDoc in pDocs:
        if term in pDoc:
            df[term] += 1

tfidfMatrix = []
for doc in docs:
    tfidfDoc = []
    maxFreq = max(doc.getFrequency().values())
    for term in uniqueTerms:
        if term in doc.getFrequency():
            tf = doc.getFrequency()[term] / maxFreq
            w = (1 + math.log10(tf)) * math.log10(N / df[term])
        else:
            tf = 0
            w = 0
        tfidfDoc.append(w)
    doc.setTFIDF(tfidfDoc)
    tfidfMatrix.append(tfidfDoc)

tfidfQuery = []
maxFreq = max(query.getFrequency().values())
for term in uniqueTerms:
    if term in query.getFrequency():
        tf = query.getFrequency()[term] / maxFreq
        w = (1 + math.log10(tf)) * math.log10(N / df[term])
    else:
        tf = 0
        w = 0
    tfidfQuery.append(w)
query.setTFIDF(tfidfQuery)

for doc in docs:
    print(doc.getName())
    print("Document:", doc.getText())
    print("Query:", query.getText())
    print(cosineSim(doc.getTFIDF(), query.getTFIDF()))
    print("================================")

"""## **MusiXMatch IR**"""

from google.colab import drive
drive.mount('/content/drive/')

import requests
api_key = "b5df0ab186f1d511c08732648816bb51"

def getTrackInfo(track_id):
  base_url = "https://api.musixmatch.com/ws/1.1/"
  endpoint = f"track.get?commontrack_id={track_id}&apikey={api_key}"

  url = base_url + endpoint
  response = requests.get(url)
  if response.status_code == 200:
    # print(response.json())
    return response.json()['message']
  else:
    return None

class TrackHandler:
    def __init__(self, trackID, freq):
        self.trackID = trackID
        self.freq = freq
        self.stopWordRemoval()

    def getTrackID(self):
        return self.trackID

    def getFrequency(self):
        return self.freq

    def setTFIDF(self, tfidf):
        self.tfidf = tfidf

    def getTFIDF(self):
        return self.tfidf

    def stopWordRemoval(self):
        stop_words = set(nltk.corpus.stopwords.words('english'))
        newFreq = {}
        for word in self.freq:
            if word in stop_words:
                continue
            else:
                newFreq[word] = self.freq[word]
        self.freq = newFreq

top_words = []
tracks = []
count = 0
with open('/content/drive/MyDrive/SEM-9/IR Lab/mxm_dataset_test.txt', 'r') as f:
  for line in f:
    if count < 17:
      count += 1
      continue
    elif count == 17:
      top_words = line.strip()[1:].split(',')
      count += 1
    else:
      tracks.append(line.strip())

# Parsing
track_list = []
for track in tracks:
  track_dict = {}
  track_details = track.split(',')
  track_dict['trackid'] = track_details[1]
  word_freq = {}
  for i in range(2,len(track_details)):
    index, count = track_details[i].split(':')
    word_freq[top_words[int(index)-1]] = int(count)
  track_dict['word_freq'] = word_freq
  track_list.append(track_dict)

print(len(track_list))
print(track_list[0])

trackObjects = []
for track in track_list:
    trackObjects.append(TrackHandler(track['trackid'], track['word_freq']))
    if(len(trackObjects) == 100):
        break

"""Get Query"""

print("Enter the query: ")
query = input()
query = DocHandler('Query', query)
pQuery = query.getPreProcessed()
print(pQuery)

"""VSM"""

import math

uniqueTerms = []
for track in trackObjects:
    for term in track.getFrequency():
        if term not in uniqueTerms:
            uniqueTerms.append(term)

print("There are totally {} unique terms\n\n".format(len(uniqueTerms)))

N = len(track_list)
df = {}

for term in uniqueTerms:
    df[term] = 0
    for track in trackObjects:
        if term in track.getFrequency():
            df[term] += 1

tfidfMatrix = []
for track in trackObjects:
    tfidfTrack = []
    maxFreq = max(track.getFrequency().values())
    for term in uniqueTerms:
        if term in track.getFrequency():
            tf = track.getFrequency()[term] / maxFreq
            w = (1 + math.log10(tf)) * math.log10(N / df[term])
        else:
            tf = 0
            w = 0
        tfidfTrack.append(w)
    track.setTFIDF(tfidfTrack)
    tfidfMatrix.append(tfidfTrack)

tfidfQuery = []
maxFreq = max(query.getFrequency().values())
for term in uniqueTerms:
    if term in query.getFrequency():
        tf = query.getFrequency()[term] / maxFreq
        w = (1 + math.log10(tf)) * math.log10(N / df[term])
    else:
        tf = 0
        w = 0
    tfidfQuery.append(w)
query.setTFIDF(tfidfQuery)

sortedTracks = sorted(trackObjects, key = lambda x: cosineSim(x.getTFIDF(), query.getTFIDF()), reverse = True)

for track in sortedTracks:
    if(track.getTrackID() == 1751988):
        print("*************")
        break
    print("Track ID:", track.getTrackID())
    print("Query:", query.getText())
    for term in query.getFrequency():
        if term in track.getFrequency():
            print(term, track.getFrequency()[term])
        else:
            print(term, 0)
    print("Cosine Similarity:", cosineSim(track.getTFIDF(), query.getTFIDF()))
    print("===========================")

i = 0
count = 0
while count < 5:
    print("Track ID:", sortedTracks[i].getTrackID())
    trackInfo = getTrackInfo(sortedTracks[i].getTrackID())
    if(trackInfo['header']['status_code'] != 200):
        print("Track not found!")
        print("--------------------------------------------------------------------------------------------------------")
        i += 1
        continue
    print("Track Name:", trackInfo['body']['track']['track_name'])
    print("Album Name:", trackInfo['body']['track']['album_name'])
    print("Artist Name:", trackInfo['body']['track']['artist_name'])
    print("URL:", trackInfo['body']['track']['track_share_url'])
    print("--------------------------------------------------------------------------------------------------------")
    i += 1
    count += 1

id = 1751988
print("Track ID:", id)
trackInfo = getTrackInfo(id)
if(trackInfo['header']['status_code'] != 200):
    print("Track not found!")
    print("----------------------------------------------")
else:
    print("Track Name:", trackInfo['body']['track']['track_name'])
    print("Album Name:", trackInfo['body']['track']['album_name'])
    print("Artist Name:", trackInfo['body']['track']['artist_name'])
    print("URL:", trackInfo['body']['track']['track_share_url'])
    print("----------------------------------------------")